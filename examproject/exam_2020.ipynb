{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
    }
   ],
   "source": [
    "# Import of packages\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import linalg\n",
    "from scipy import optimize\n",
    "from scipy import interpolate\n",
    "import sympy as sm\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# autoreload modules when code is run\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the following **linear equation:**\n",
    "\n",
    "$$y_i = \\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i} + \\epsilon_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume you have access to data of the **independent variables** ($x_{1,i}$, $x_{2,i}$) and the **dependent variable** ($y_i$) for $N$ individuals, where $i$ indexes individuals. The variable $\\epsilon_i$ is a mean-zero **stochastic shock**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume the **data generating process** is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DGP(N):\n",
    "    \n",
    "    # a. independent variables\n",
    "    x1 = np.random.normal(0,1,size=N)\n",
    "    x2 = np.random.normal(0,1,size=N)\n",
    "    \n",
    "    # b. errors\n",
    "    eps = np.random.normal(0,1,size=N)\n",
    "    \n",
    "    extreme = np.random.uniform(0,1,size=N)\n",
    "    eps[extreme < 0.05] += np.random.normal(-5,1,size=N)[extreme < 0.05]\n",
    "    eps[extreme > 0.95] += np.random.normal(5,1,size=N)[extreme > 0.95]\n",
    "    \n",
    "    # c. dependent variable\n",
    "    y = 0.1 + 0.3*x1 + 0.5*x2 + eps\n",
    "    \n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The data you have access to is:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2020)\n",
    "x1,x2,y = DGP(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Estimate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using **ordinary least squares (OLS)** implemented with **matrix algebra** by\n",
    "\n",
    "$$ \\hat{\\mathbf{\\beta}} = (\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\\mathbf{X}^{\\prime}\\mathbf{y} $$\n",
    "\n",
    "where $\\mathbf{X}^{\\prime}$ is the transpose of $\\mathbf{X}$ and\n",
    "\n",
    "$$\\mathbf{y} = \n",
    "\\pmatrix{ y_1 \\cr y_2 \\cr  \\vdots \\cr y_N \n",
    "}\n",
    ", \\quad \\mathbf{X} = \\pmatrix{\n",
    "1 & x_{1,1} & x_{2,1} \\cr \n",
    "1 & x_{1,2} & x_{2,2} \\cr \n",
    "\\vdots & \\vdots \\cr \n",
    "1 & x_{1,N} & x_{2,N} \n",
    "}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the matrix X\n",
    "a = [1]*10000\n",
    "X = np.matrix(np.vstack((a,x1,x2)))\n",
    "print('stacked:\\n',X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transposing and inversing the matrix X according to the beta_hat given\n",
    "x = X.transpose()\n",
    "X_mul=X @ x\n",
    "X_mul_inv=np.linalg.inv(X_mul)\n",
    "x_mul=X_mul_inv@X\n",
    "y1=y[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimating the vector of coefficients using OLS \n",
    "beta=x_mul@y1\n",
    "beta0_hat=float(beta[0])\n",
    "beta1_hat=float(beta[1])\n",
    "beta2_hat=float(beta[2])\n",
    "print(f'beta0_hat = {beta0_hat:.3f}, beta1_hat = {beta1_hat:.3f}, beta2_hat = {beta2_hat:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Construct a 3D plot, where the data is plotted as scattered points, and the prediction of the model is given by the plane\n",
    "\n",
    "$$\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_{1,i} + \\hat{\\beta}_2 x_{2,i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predicting the model given the values obtained in Q1 \n",
    "beta0_predict=[beta0_hat]*10000\n",
    "x1_predict=beta1_hat*x1\n",
    "x2_predict=beta2_hat*x2\n",
    "y_predict = beta0_predict + x1_predict + x2_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating 3D Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. main\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "cs = ax.scatter(x1_predict,x2_predict, y_predict);        \n",
    "\n",
    "# b. add labels\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$y$')\n",
    "\n",
    "# c. invert xaxis\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# d. colorbar\n",
    "fig.colorbar(cs);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** Esimtate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using a **numerical solver** to solve the ordinary least square problem, shown below, directly. Compare your results with the matrix algebra results.\n",
    "\n",
    "$$ \\min_{\\mathbf{\\beta}} \\sum^N_{i=1} (y_i - (\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}) )^2 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initially defining the residuals\n",
    "u = np.empty(len(x1))\n",
    "def residuals(beta0,beta1,beta2):\n",
    "    for i in range(len(x1)):\n",
    "        u[i] = (y1[i] - beta0 - beta1*x1[i] - beta2*x2[i])\n",
    "    return u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-f637d91dde28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m#Printing the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'beta0 = {res.x[0]:.3f}, beta1 = {res.x[1]:.3f}, beta2 = {res.x[2]:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "#Defining objective function\n",
    "def OLS_objective_function(x,beta0,beta1,beta2):\n",
    "    beta0=x[0]\n",
    "    beta1=x[1]\n",
    "    beta2=x[2]\n",
    "    OLS_sum_squared_errors = np.sum(residuals(beta0,beta1,beta2)**2)\n",
    "    return OLS_sum_squared_errors\n",
    "    \n",
    "    #Initial guess of betas\n",
    "    beta_guess= np.array([1,1,1]) \n",
    "   \n",
    "    #Calling a solver\n",
    "    res = optimize.minimize(OLS_objective_function,beta_guess,method='Nelder-Mead',args=(beta0,beta1,beta2))\n",
    "\n",
    "#Printing the results\n",
    "print(f'beta0 = {res.x[0]:.3f}, beta1 = {res.x[1]:.3f}, beta2 = {res.x[2]:.3f}') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Comparing the results from the numerical solver with that of the matrix algebra **\n",
    "\n",
    "The comparison reveals that we down to the fourth decimal obtain the exact same result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 4:** Estimate the vector of coefficients $\\mathbf{\\beta} = (\\beta_0,\\beta_1,\\beta_2)$ using **least absolute deviations (LAD)** using a numerical solver to solve the following problem directly: \n",
    "\n",
    "$$  \\min_{\\beta} \\sum^N_{i=1} |y_i - (\\beta_0 + \\beta_1 x_{1,i} + \\beta_2 x_{2,i}) | $$\n",
    "\n",
    "where $|z|$ is the absolute value of $z$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-1702fb8830b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLAD_objective_function\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta_guess\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Nelder-Mead'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'beta0 = {res.x[0]:.3f}, beta1 = {res.x[1]:.3f}, beta2 = {res.x[2]:.3f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    }
   ],
   "source": [
    "#Defining a new objective function\n",
    "def LAD_objective_function(x,beta0,beta1,beta2):\n",
    "    beta0=x[0]\n",
    "    beta1=x[1]\n",
    "    beta2=x[2]\n",
    "    LAD_sum_squared_errors = np.sum(np.absolute(residuals(beta0,beta1,beta2)))\n",
    "    return LAD_sum_squared_errors    \n",
    "\n",
    "    res = optimize.minimize(LAD_objective_function,beta_guess,method='Nelder-Mead',args=(beta0,beta1,beta2))\n",
    "\n",
    "print(f'beta0 = {res.x[0]:.3f}, beta1 = {res.x[1]:.3f}, beta2 = {res.x[2]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 5:** Set $N = 50$. Repeat the estimation using the **OLS** and **LAD** methods $K=5000$ times, drawing a new random sample from the data generating process each time. Compare the estimates from each method using histograms. Which method do you prefer? Explain your choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** OLS Method **\n",
    "\n",
    "Repeating the estimation using the OLS method 5000 times, drawing a new random sample from the data generating process each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting K=5000\n",
    "beta0_OLS = np.empty(5000)\n",
    "beta1_OLS = np.empty(5000)\n",
    "beta2_OLS = np.empty(5000)\n",
    "\n",
    "#Setting N=50 and K=5000\n",
    "for i in range(5000):\n",
    "    x1_Q5,x2_Q5,y_Q5 = DGP(50)\n",
    "    \n",
    "    #Defining new residuals\n",
    "    u_Q5=np.empty(len(x1_Q5))\n",
    "    def residuals_Q5(beta0_OLS,beta1_OLS,beta2_OLS):\n",
    "        for i in range(len(x1_Q5)):\n",
    "            u_Q5[i] = (y_Q5[i] - beta0_OLS - beta1_OLS*x1_Q5[i] - beta2_OLS*x2_Q5[i])\n",
    "        return u_Q5\n",
    "    \n",
    "    #Defining new objective function\n",
    "    def objective_function_Q5_OLS(x,beta0_OLS,beta1_OLS,beta2_OLS):\n",
    "        beta0_OLS=x[0]\n",
    "        beta1_OLS=x[1]\n",
    "        beta2_OLS=x[2]\n",
    "        sum_squared_errors_Q5_OLS = np.sum(residuals_Q5(beta0_OLS,beta1_OLS,beta2_OLS)**2)\n",
    "        return sum_squared_errors_Q5_OLS\n",
    "\n",
    "    #Initial guess of betas\n",
    "    beta_guess= np.array([1,1,1]) \n",
    "\n",
    "    #Calling a solver \n",
    "    res = optimize.minimize(objective_function_Q5_OLS,beta_guess,method='Nelder-Mead',args=(beta0_OLS,beta1_OLS,beta2_OLS))\n",
    "    beta0_OLS[i-1]=res.x[0]\n",
    "    beta1_OLS[i-1]=res.x[1]\n",
    "    beta2_OLS[i-1]=res.x[2]\n",
    "\n",
    "#Printing the results\n",
    "print(res.x[0])\n",
    "print(res.x[1])\n",
    "print(res.x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** LAD Method **\n",
    "\n",
    "Repeating the estimation using the LAD method 5000 times, drawing a new random sample from the data generating process each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting K=5000\n",
    "beta0_LAD = np.empty(5000)\n",
    "beta1_LAD = np.empty(5000)\n",
    "beta2_LAD = np.empty(5000)\n",
    "\n",
    "#Setting N=50 and K=5000\n",
    "for i in range(5000):\n",
    "    x1_Q5,x2_Q5,y_Q5 = DGP(50)\n",
    "    \n",
    "    #Defining new residuals\n",
    "    u_Q5=np.empty(len(x1_Q5))\n",
    "    def residuals_Q5(beta0_LAD,beta1_LAD,beta2_LAD):\n",
    "        for i in range(len(x1_Q5)):\n",
    "            u_Q5[i] = (y_Q5[i] - beta0_LAD - beta1_LAD*x1_Q5[i] - beta2_LAD*x2_Q5[i])\n",
    "        return u_Q5\n",
    "   \n",
    "    #Defining new objective function\n",
    "    def objective_function_Q5(x,beta0_LAD,beta1_LAD,beta2_LAD):\n",
    "        beta0_LAD=x[0]\n",
    "        beta1_LAD=x[1]\n",
    "        beta2_LAD=x[2]\n",
    "        sum_squared_errors_Q5_LAD = np.sum(np.abs(residuals_Q5(beta0_LAD,beta1_LAD,beta2_LAD)))\n",
    "        return sum_squared_errors_Q5_LAD\n",
    "\n",
    "    #Calling a solver \n",
    "    res = optimize.minimize(objective_function_Q5,beta_guess,method='Nelder-Mead',args=(beta0_LAD,beta1_LAD,beta2_LAD))\n",
    "    beta0_LAD[i-1]=res.x[0]\n",
    "    beta1_LAD[i-1]=res.x[1]\n",
    "    beta2_LAD[i-1]=res.x[2]\n",
    "\n",
    "#Printing the results\n",
    "print(res.x[0])\n",
    "print(res.x[1])\n",
    "print(res.x[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Comparing the estimates from the OLS and LAD methods using histograms **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Histogram for $\\beta_0$ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig0 = plt.figure()\n",
    "ax = fig0.add_subplot(1,1,1)\n",
    "\n",
    "ax.hist(beta0_OLS,bins=100,label='OLS')\n",
    "ax.hist(beta0_LAD,bins=100,label='LAD')\n",
    "\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True)\n",
    "ax.set_title('Comparing beta_0 results from OLS and LAD method');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Histogram for $\\beta_1$ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure()\n",
    "ax = fig1.add_subplot(1,1,1)\n",
    "\n",
    "ax.hist(beta1_OLS,bins=100,label='OLS')\n",
    "ax.hist(beta1_LAD,bins=100,label='LAD')\n",
    "\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True)\n",
    "ax.set_title('Comparing beta_1 results from OLS and LAD method');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Histogram for $\\beta_2$ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure()\n",
    "ax = fig2.add_subplot(1,1,1)\n",
    "\n",
    "ax.hist(beta2_OLS,bins=100,label='OLS')\n",
    "ax.hist(beta2_LAD,bins=100,label='LAD')\n",
    "\n",
    "ax.legend(loc='lower right',facecolor='white',frameon=True)\n",
    "ax.set_title('Comparing beta_2 results from OLS and LAD method');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Preferred method **\n",
    "\n",
    "We prefer the LAD method to the OLS method, as it gives a smaller variance. This is evident from the first two graphs, as the LAD and OLS histograms have roughly the same height. That means that the there's a large amount of the data around the true paramter value, $\\hat{\\beta}$. However, the LAD model has a smaller variance, evident from the slimmer widths in the plots, meaning that our estimates are close to the true parameter values of the betas. This is what drives our conclusion that the LAD model is preferred. \n",
    "\n",
    "However, we note that in the 3rd historgram, plotting the $\\beta_2$ values, the OLS histogram exhibits a significantly higher curve than the OLS. Thus, one could argue that the conclusion is more ambiguous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Durable purchases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider a **household** living in two periods.\n",
    "\n",
    "In the **second period** it gets utility from **non-durable consumption**, $c$, and **durable consumption**, $d+\\chi x$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{2}(m_{2},d)&= \\max_{c}\\frac{(c^{\\alpha}(d+\\chi x)^{1-\\alpha})^{1-\\rho}}{1-\\rho}\\\\\n",
    "\\text{s.t.} \\\\\n",
    "x &= m_{2}-c \\\\\n",
    "c &\\in [0,m_{2}]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "* $m_2$ is cash-on-hand in the beginning of period 2\n",
    "* $c$ is non-durable consumption\n",
    "* $d$ is pre-commited durable consumption\n",
    "* $x = m_2 - c$ is extra durable consumption\n",
    "* $\\rho > 1$ is the risk aversion coefficient\n",
    "* $\\alpha \\in (0,1)$ is the utility weight on non-durable consumption\n",
    "* $\\chi \\in (0,1)$ implies that extra durable consumption is *less* valuable than pre-comitted durable consumption\n",
    "* the second constraint ensures the household *cannot* die in debt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **value function** $v_2(m_2,d)$ measures the household's value of having $m_2$ at the beginning of period 2 with precomitted durable consumption of $d$. The optimal choice of non-durable consumption is denoted $c^{\\ast}(m_2,d)$. The optimal extra durable consumption function is $x^{\\ast}(m_2,d) = m_2-c^{\\ast}(m_2,d)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the so-called **end-of-period 1 value function** as:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w(a,d)&\\equiv\\beta\\mathbb{E}_{1}\\left[v_2(m_2,d)\\right]\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m_2&= (1+r)a+y \\\\\n",
    "y &= \\begin{cases}\n",
    "1-\\Delta & \\text{with prob. }\\frac{1}{3}\\\\\n",
    "1 & \\text{with prob. }\\frac{1}{3}\\\\\n",
    "1+\\Delta & \\text{with prob. }\\frac{1}{3}\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "* $a$ is assets at the end of period 1\n",
    "* $\\beta > 0$ is the discount factor\n",
    "* $\\mathbb{E}_1$ is the expectation operator conditional on information in period 1\n",
    "* $y$ is income in period 2\n",
    "* $\\Delta \\in (0,1)$ is the level of income risk (mean-preserving)\n",
    "* $r$ is the return on savings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the **first period**, the household chooses it's pre-comitted level of durable consumption for the next-period,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{1}(m_{1})&=\\max_{d} w(a,d)\\\\&\\text{s.t.}&\\\\\n",
    "a&= m_{1}-d \\\\\n",
    "d&\\in [0,m_{1}]\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $m_1$ is cash-on-hand in period 1. The second constraint ensures the household *cannot* borrow. The **value function** $v_1(m_1)$ measures the household's value of having $m_1$ at the beginning of period 1. The optimal choice of pre-committed durable consumption is denoted $d^{\\ast}(m_1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **parameters** and **grids** for $m_1$, $m_2$ and $d$ should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. parameters\n",
    "rho = 2\n",
    "alpha = 0.8\n",
    "beta = 0.96\n",
    "r = 0.04\n",
    "Delta = 0.25\n",
    "chi =0.9\n",
    "\n",
    "# b. grids\n",
    "m1_vec = np.linspace(1e-8,10,100)\n",
    "m2_vec = np.linspace(1e-8,10,100)\n",
    "d_vec = np.linspace(1e-8,5,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** Find and plot the functions $v_{2}(m_{2},d)$, $c^{\\ast}(m_2,d)$, and $x^{\\ast}(m_2,d)$. Comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def m2(r, a, y):\n",
    "    if y==1-Delta:\n",
    "        return (1+r)*a+(1-Delta)\n",
    "    elif y==1:\n",
    "        return (1+r)*a+1\n",
    "    else: \n",
    "        return (1+r)*a+(1+Delta)\n",
    "\n",
    "def ex_dur_con(m2,c):\n",
    "    return m2-c\n",
    "\n",
    "def assets1(m1,d):\n",
    "    return m1-d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def v2(m2,c,rho,d,chi,alpha):\n",
    "    return (c**alpha*(d+chi*ex_dur_con(m2,c))**(1-alpha))**(1-rho)/(1-rho)\n",
    "\n",
    "def v1(m1, d, beta, Delta, r, v2_interp):\n",
    "    # a. v2 value, if low income\n",
    "    m2_low = (1+r)*(assets1(m1,d)) + (1-Delta)\n",
    "    v2_low = v2_interp((m2_low, d_vec))[0]\n",
    "    # b. v2 value, if medium income\n",
    "    m2_medium = (1+r)*(assets1(m1,d)) + 1\n",
    "    v2_medium = v2_interp((m2_medium,d_vec))[0]\n",
    "    # c. v2 value, if high income\n",
    "    m2_high = (1+r)*(assets1(m1,d))+(1+Delta)\n",
    "    v2_high = v2_interp((m2_high,d_vec))[0]\n",
    "    # d. expected v2 value\n",
    "    v2 = (1/3)*v2_low + (1/3)*v2_medium + (1/3)*v2_high\n",
    "    # e. total value\n",
    "    return beta*v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_period_2(alpha, rho, chi, Delta):\n",
    "\n",
    "    #a. Grids\n",
    "    v2_vec=np.empty((100,100))\n",
    "    c_vec=np.empty((100,100))\n",
    "\n",
    "    #b. solve for each m2 in grid\n",
    "    for i,m2 in enumerate(m2_vec):\n",
    "        for j,d in enumerate(d_vec):\n",
    "\n",
    "            #i. Objective\n",
    "            objective = lambda c: -v2(m2,c,rho,d,chi,alpha)\n",
    "\n",
    "            #ii. Initial value (consuming half)\n",
    "            x0=m2/2\n",
    "\n",
    "            #iii. Optimizer\n",
    "            result = optimize.minimize_scalar(objective,x0,method='bounded', bounds=[1e-8,m2])\n",
    "\n",
    "            #iv. save\n",
    "            v2_vec[i,j]= -result.fun\n",
    "            c_vec[i,j]=result.x\n",
    "    \n",
    "    return m2_vec, d_vec, v2_vec, c_vec\n",
    "\n",
    "# Solve\n",
    "m2_vec, d_vec, v2_vec, c_vec = solve_period_2(alpha,rho,chi,Delta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**  Value Function in Period 2, $v_2(m_2,d)$ ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting the figure in 2D\n",
    "fig0=plt.figure()\n",
    "ax=fig0.add_subplot(1,1,1)\n",
    "ax.plot(m2_vec,v2_vec)\n",
    "\n",
    "# Adding labels to plot\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$v_2$')\n",
    "\n",
    "ax.set_title('Value Function in Period 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When cash-on-hand is 0 in Period 2 the value function exhibits negative curvature. When cash-on-hand exceeds 0 it the value function increases. This suggests that the more cash-on-hand you have in Period 2, you will have a constant value value function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Optimal Consumption in Period 2, $c^*(m_2,d)$ ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grids\n",
    "m2_grid,d_grid=np.meshgrid(m2_vec,d_vec,indexing='ij')\n",
    "\n",
    "# Plotting the figure in 3D\n",
    "fig1 = plt.figure()\n",
    "ax = fig1.add_subplot(1,1,1,projection='3d')\n",
    "c = ax.plot_surface(m2_grid, d_grid, c_vec, cmap=cm.jet)\n",
    "\n",
    "# Adding labels to plot\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('d')\n",
    "ax.set_zlabel('$c$')\n",
    "\n",
    "ax.set_title('Consumption Function in Period 2')\n",
    "\n",
    "# Inverting axis\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# Adding colourbar\n",
    "fig1.colorbar(c);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we see that the variable $d$, the pre-committed durable good, does not affect the consumption, while the cash-on-hand in the second period affects the consumption positively. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Optimal Extra Durable Consumption in Period 2, $x^*(m_2,d)$ ** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grids\n",
    "m2_grid,d_grid = np.meshgrid(m2_vec,d_vec,indexing='ij')\n",
    "x_vec = m2_vec - c_vec\n",
    "\n",
    "# Plotting the figure in 3D\n",
    "fig2 = plt.figure()\n",
    "ax = fig2.add_subplot(1,1,1,projection='3d')\n",
    "x = ax.plot_surface(m2_grid,d_grid,x_vec,cmap=cm.jet)\n",
    "\n",
    "# Adding labels\n",
    "ax.set_xlabel('$m_2$')\n",
    "ax.set_ylabel('$d$')\n",
    "ax.set_zlabel('$x$')\n",
    "ax.set_title('Extra Durable Consumption Function in Period 2')\n",
    "\n",
    "# Inverting the xaxis\n",
    "ax.invert_xaxis()\n",
    "\n",
    "# Adding colourbar\n",
    "fig2.colorbar(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows a, to us, interesting and surprising result: Increasing cash-on-hand gives less extra durable consumption. We were expecting the opposite result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 2:** Find and plot the functions $v_{1}(m_{1})$ and $d^{\\ast}(m_1)$. Comment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plotting Value function for period 1 $v_1(m_1)$ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining Period 1 and 2\n",
    "v2_interp = interpolate.RegularGridInterpolator((m2_vec,d_vec),v2_vec,\n",
    "                                                bounds_error=False,fill_value=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling a solver\n",
    "def solve_period_1(beta,Delta,r,v2_interp):\n",
    "\n",
    "    # a. Defining grids for Period 1\n",
    "    m1_vec = np.linspace(1e-8,10,100)\n",
    "    v1_vec = np.empty((100,100))\n",
    "    d1_vec = np.empty((100,100))\n",
    "\n",
    "    # b. Solving for each m1 in the grids\n",
    "    for i,m1 in enumerate(m1_vec):\n",
    "            \n",
    "            # i. objective\n",
    "            objective2 = lambda d: -v1(m1,d,beta,Delta,r,v2_interp)\n",
    "            \n",
    "            # ii. initial value (consume half)\n",
    "            x0 = m1/2\n",
    "            \n",
    "            # iii. optimizer\n",
    "            result2 = optimize.minimize_scalar(objective2,x0,method='bounded',bounds=[1e-8,m1])\n",
    "            \n",
    "            # iv. save\n",
    "            v1_vec[i] = -result2.fun\n",
    "            d1_vec[i] = result2.x\n",
    "    \n",
    "    return m1_vec,v1_vec,d1_vec\n",
    "\n",
    "# solve\n",
    "m1_vec,v1_vec,d1_vec = solve_period_1(beta,Delta,r,v2_interp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the figure in 2D\n",
    "fig3 = plt.figure()\n",
    "ax = fig3.add_subplot(1,1,1)\n",
    "ax.plot(m1_vec,v1_vec)\n",
    "\n",
    "# Adding labels to plot\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$value$')\n",
    "ax.set_title('Value Function in Period 1')\n",
    "ax.set_ylim([-2,0]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot exhibits a positive relationship. When cash-on-hand in period 1, $m_1$, increases the Value Function's increases too. Implying that the value increases when you have more cash-on-hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Plotting Durable Consumption for period 1 $d^*(m_1)$ **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the figure in 2D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.plot(m1_vec, d1_vec)\n",
    "\n",
    "# Adding labels to plot\n",
    "ax.set_xlabel('$m_1$')\n",
    "ax.set_ylabel('$d$')\n",
    "ax.set_title('Durable Goods Consumption in Period 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For low values of $m_1$ the durables goods consumption, d, increases. It increases until the approx. 0.000006 at which point it decreases until approx. 0.000003. It then continues to fluctuate between these two points increasing in $m_1$. \n",
    "\n",
    "This pattern suggests that up until a certain point the desire to save for Period 2 is increasing, then starting to fluctuate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider an **extension** of the model, where there is also a **period 0**. In this period, the household makes a choice whether to stick with the level of durables it has, $z = 0$, or adjust its stock of durables, $z = 1$. If adjusting, the household loses a part of the value of its durable stock; more specificaly it incurs a proportional loss of $\\Lambda \\in (0,1)$.\n",
    "\n",
    "Mathematically, the **household problem in period 0** is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v_{0}(m_{0},d_{0}) &= \\max_{z\\in\\{0,1\\}} \\begin{cases}\n",
    "w(m_{0},d_{0}) & \\text{if } z = 0\\\\\n",
    "v_1(m_0+(1-\\Lambda) d_{0}) & \\text{if } z = 1\\\\\n",
    "\\end{cases}\\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **parameters** and **grids** for $m_0$ and $d_0$ should be:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 0.2\n",
    "m0_vec = np.linspace(1e-8,6,100)\n",
    "d0_vec = np.linspace(1e-8,3,100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 3:** For which values of $m_0$ and  $d_0$ is the optimal choice not to adjust, i.e. $z = 0$? Show this in a plot. Give an interpretion of your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\boldsymbol{x} = \\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2\\\\\n",
    "\\end{array}\\right]$ be a two-dimensional vector. Consider the following algorithm:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm:** `gradient_descent()`\n",
    "\n",
    "**Goal:** Minimize the function $f(\\boldsymbol{x})$.\n",
    "\n",
    "1. Choose a tolerance $\\epsilon>0$, a scale factor $ \\Theta > 0$, and a small number $\\Delta > 0$\n",
    "2. Guess on $\\boldsymbol{x}_0$ and set $n=1$\n",
    "3. Compute a numerical approximation of the jacobian for $f$ by\n",
    "\n",
    "    $$\n",
    "    \\nabla f(\\boldsymbol{x}_{n-1}) \\approx \\frac{1}{\\Delta}\\left[\\begin{array}{c}\n",
    "    f\\left(\\boldsymbol{x}_{n-1}+\\left[\\begin{array}{c}\n",
    "    \\Delta\\\\\n",
    "    0\n",
    "    \\end{array}\\right]\\right)-f(\\boldsymbol{x}_{n-1})\\\\\n",
    "    f\\left(\\boldsymbol{x}_{n-1}+\\left[\\begin{array}{c}\n",
    "    0\\\\\n",
    "    \\Delta\n",
    "    \\end{array}\\right]\\right)-f(\\boldsymbol{x}_{n-1})\n",
    "    \\end{array}\\right]\n",
    "    $$\n",
    "\n",
    "4. Stop if the maximum element in $|\\nabla f(\\boldsymbol{x}_{n-1})|$ is less than $\\epsilon$\n",
    "5. Set $\\theta = \\Theta$ \n",
    "6. Compute $f^{\\theta}_{n} = f(\\boldsymbol{x}_{n-1} - \\theta \\nabla f(\\boldsymbol{x}_{n-1}))$\n",
    "7. If $f^{\\theta}_{n} < f(\\boldsymbol{x}_{n-1})$ continue to step 9\n",
    "8. Set $\\theta = \\frac{\\theta}{2}$ and return to step 6     \n",
    "9. Set $x_{n} = x_{n-1} - \\theta \\nabla f(\\boldsymbol{x}_{n-1})$\n",
    "10. Set $n = n + 1$ and return to step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Implement the algorithm above such that the code below can run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting symbols\n",
    "x1 = sm.symbols('x_1')\n",
    "x2 = sm.symbols('x_2')\n",
    "delta = sm.symbols('delta')\n",
    "epsilon = sm.symbols('epsilon')\n",
    "Theta = sm.symbols('Theta')\n",
    "theta = sm.symbols('theta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rosenbrock Function is defined and a matrix for the Jacobian is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ros(x1,x2):\n",
    "    f = (1.0-x1)**2 + 2*(x2-x1**2)**2\n",
    "f = (1.0-x1)**2 + 2*(x2-x1**2)**2\n",
    "\n",
    "jacmatrix = sm.Matrix([sm.diff(f,i) for i in [x1,x2]])\n",
    "\n",
    "jacmatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The numerical approximation of the jacobian for $f$ is computed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 3\n",
    "def rosen(x):\n",
    "    return ros(x[0],x[1])\n",
    "\n",
    "def rosenjac(x):\n",
    "    return np.array([-(2.0-2*x[0])-8*x[0]*(x[1]-x[0]**2),4*(x[1]-x[0]**2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the algorithm\n",
    "\n",
    "In the following we implement the 10 steps to minimize the function $f(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Choosing tolerance, scale factor and a small number\n",
    "def gradient_descent(f,x0,epsilon=1e-6,Theta=0.1,Delta=1e-8,max_iter=10_000):\n",
    "\n",
    "    # 2. Guessing on x_0 and setting n=1\n",
    "    x = x0\n",
    "    fx = f(x0)\n",
    "    n = 1\n",
    "\n",
    "    while n < max_iter: \n",
    "        x_prev = x\n",
    "        fx_prev = fx\n",
    "\n",
    "        # 3. Computing the numerical approximation of the jacobian\n",
    "        jacx = rosenjac(x)\n",
    "\n",
    "        # 4. - 5. Defining new variables  \n",
    "        fx_ast = np.inf\n",
    "        theta_ast = Theta\n",
    "        \n",
    "        #6. Implementing stepsize \n",
    "        theta=Theta/2 \n",
    "\n",
    "        #7. Continuing the loop\n",
    "        if fx < fx_ast:\n",
    "            fx_ast = fx\n",
    "            theta_ast = theta\n",
    "\n",
    "        # 8. Last step of the minimization \n",
    "        x = x_prev - theta_ast*jacx\n",
    "        fx = f(x)\n",
    "        if abs(fx-fx_prev) < Delta:\n",
    "            break\n",
    "        # 9. Implementing restriction \n",
    "        fx = f(x)\n",
    "        if abs(fx<fx_prev) < epsilon:\n",
    "            break\n",
    "                \n",
    "        # 10. Finishing loop or returning to step 3\n",
    "        n += 1\n",
    "        \n",
    "    return x,n\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test case:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rosen(x):\n",
    "    return (1.0-x[0])**2+2*(x[1]-x[0]**2)**2\n",
    "\n",
    "x0 = np.array([1.1,1.1])\n",
    "try:\n",
    "    x,it = gradient_descent(rosen,x0)\n",
    "    print(f'Minimum found at ({x[0]:.4f},{x[1]:.4f}) after {it} iterations')\n",
    "    assert np.allclose(x,[1,1])\n",
    "except:\n",
    "    print('Not implemented yet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}